{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import warnings\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.externals import joblib\n",
    "from ItemSelector import ItemSelector\n",
    "from label_rank import label_rank\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning,\n",
    "                        module='lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input parameters.\n",
    "One of the most immportant parameters is the number of estimators that allows you to trade-off accuracy and modeling time. The table below should give you an idea of the relationships between the number of estimators and the metrics.\n",
    "\n",
    "| estimators | run time (s) | Accuracy@1 | Accuracy@2 | Accuracy@3 |\n",
    "|------------|--------------|----------|--------|--------|\n",
    "|        100 |           40 |   25.02% | 38.72% | 47.83% |\n",
    "|       1000 |          177 |   46.79% | 60.80% | 69.11% |\n",
    "|       2000 |          359 |   51.38% | 65.93% | 73.09% |\n",
    "|       4000 |          628 |   53.39% | 67.40% | 74.74% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_data = 'balanced_pairs_train.tsv'\n",
    "args_test = 'balanced_pairs_test.tsv'\n",
    "args_estimators = 4000\n",
    "args_ngrams = 1\n",
    "args_unweighted = False\n",
    "args_min_child_samples = 20\n",
    "args_match = 20\n",
    "args_outputs = '.'\n",
    "args_inputs = '.'\n",
    "args_save = True\n",
    "args_model = 'model.pkl'\n",
    "args_instances = 'inst.txt'\n",
    "args_labels = 'labels.txt'\n",
    "args_rank = 3\n",
    "args_verbose = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the paths to the input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data.\n",
    "inputs_path = args_inputs\n",
    "data_path = os.path.join(inputs_path, args_data)\n",
    "test_path = os.path.join(inputs_path, args_test)\n",
    "\n",
    "# The output data.\n",
    "outputs_path = args_outputs\n",
    "model_path = os.path.join(outputs_path, args_model)\n",
    "instances_path = os.path.join(outputs_path, args_instances)\n",
    "labels_path = os.path.join(outputs_path, args_labels)\n",
    "\n",
    "# Create the outputs folder.\n",
    "os.makedirs(outputs_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and set up the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading .\\balanced_pairs_train.tsv\n",
      "train: 132,500 rows with 5.00% matches\n"
     ]
    }
   ],
   "source": [
    "# Load the training data.\n",
    "print('Reading {}'.format(data_path))\n",
    "train = pd.read_csv(data_path, sep='\\t', encoding='latin1')\n",
    "\n",
    "# Limit the number of training duplicate matches.\n",
    "train = train[train.n < args_match]\n",
    "\n",
    "# The input data columns.\n",
    "feature_columns = ['Text_x', 'Text_y']\n",
    "label_column = 'Label'\n",
    "group_column = 'Id_x'\n",
    "answerid_column = 'AnswerId_y'\n",
    "name_columns = ['Id_x', 'Id_y']\n",
    "\n",
    "# Report on the dataset.\n",
    "print('train: {:,} rows with {:.2%} matches'.format(\n",
    "    train.shape[0], train[label_column].mean()))\n",
    "\n",
    "# Compute instance weights.\n",
    "weight_column = 'Weight'\n",
    "if args_unweighted:\n",
    "    weight = pd.Series([1.0], train[label_column].unique())\n",
    "else:\n",
    "    label_counts = train[label_column].value_counts()\n",
    "    weight = train.shape[0]/(label_counts.shape[0]*label_counts)\n",
    "train[weight_column] = train[label_column].apply(lambda x: weight[x])\n",
    "\n",
    "# Collect the ordered AnswerId.\n",
    "labels = sorted(train[answerid_column].unique())\n",
    "label_order = pd.DataFrame({'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and format the training data.\n",
    "train_X = train[feature_columns]\n",
    "train_y = train[label_column]\n",
    "sample_weight = train[weight_column]\n",
    "groups = train[group_column]\n",
    "names = train[name_columns]\n",
    "\n",
    "# Select the training hyperparameters.\n",
    "n_estimators = args_estimators\n",
    "min_child_samples = args_min_child_samples\n",
    "estimator = lgb.LGBMClassifier(n_estimators=n_estimators,\n",
    "                               min_child_samples=min_child_samples,\n",
    "                               verbose=args_verbose)\n",
    "if args_ngrams > 0:\n",
    "    ngram_range = (1, args_ngrams)\n",
    "else:\n",
    "    ngram_range = None\n",
    "assert ngram_range is not None\n",
    "\n",
    "# The featurization pipeline(s) for each text column.\n",
    "featurization = [\n",
    "    (column,\n",
    "     make_pipeline(ItemSelector(column),\n",
    "                   text.TfidfVectorizer(ngram_range=ngram_range)))\n",
    "    for column in feature_columns]\n",
    "features = FeatureUnion(featurization)\n",
    "\n",
    "# The model pipeline.\n",
    "model = Pipeline([\n",
    "    ('features', features),\n",
    "    ('model', lgb.LGBMClassifier(n_estimators=n_estimators))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('Text_x', Pipeline(memory=None,\n",
       "     steps=[('itemselector', ItemSelector(keys='Text_x')), ('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf...0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_X, train_y, model__sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the model to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_save:\n",
    "    joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading .\\balanced_pairs_test.tsv\n",
      "test 297,570 rows with 0.55% matches\n"
     ]
    }
   ],
   "source": [
    "# Read the test data.\n",
    "print('Reading {}'.format(test_path))\n",
    "test = pd.read_csv(test_path, sep='\\t', encoding='latin1')\n",
    "print('test {:,} rows with {:.2%} matches'.format(\n",
    "    test.shape[0], test[label_column].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_X = test[feature_columns]\n",
    "test['probabilities'] = model.predict_proba(test_X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame with one row per duplicate question, and make it contain the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the testing data by dupe Id and question AnswerId.\n",
    "test.sort_values([group_column, answerid_column], inplace=True)\n",
    "\n",
    "# Extract the ordered probabilities.\n",
    "probabilities = (\n",
    "    test.probabilities\n",
    "    .groupby(test[group_column], sort=False)\n",
    "    .apply(lambda x: tuple(x.values)))\n",
    "\n",
    "# Get the individual records.\n",
    "output_columns_x = ['Id_x', 'AnswerId_x', 'Text_x']\n",
    "test_score = (test[output_columns_x]\n",
    "              .drop_duplicates()\n",
    "              .set_index(group_column))\n",
    "test_score['probabilities'] = probabilities\n",
    "test_score.reset_index(inplace=True)\n",
    "test_score.columns = ['Id', 'AnswerId', 'Text', 'probabilities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy @1 = 53.39%\n",
      "Accuracy @2 = 67.40%\n",
      "Accuracy @3 = 74.74%\n",
      "Mean Rank 5.9786\n"
     ]
    }
   ],
   "source": [
    "# Rank the correct answers.\n",
    "test_score['Ranks'] = test_score.apply(lambda x:\n",
    "                                       label_rank(x.AnswerId,\n",
    "                                                  x.probabilities,\n",
    "                                                  label_order.label),\n",
    "                                       axis=1)\n",
    "\n",
    "# Compute the number of correctly ranked answers\n",
    "for i in range(1, args_rank+1):\n",
    "    print('Accuracy @{} = {:.2%}'.format(\n",
    "        i, (test_score['Ranks'] <= i).mean()))\n",
    "mean_rank = test_score['Ranks'].mean()\n",
    "print('Mean Rank {:.4f}'.format(mean_rank))\n",
    "\n",
    "# Write the scored instances.\n",
    "test_score.to_csv(instances_path, sep='\\t', index=False,\n",
    "                  encoding='latin1')\n",
    "label_order.to_csv(labels_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployment]",
   "language": "python",
   "name": "conda-env-MLAKSDeployment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
