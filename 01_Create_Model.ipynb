{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## Load libraries and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.externals import joblib\n",
    "from ItemSelector import ItemSelector\n",
    "from label_rank import label_rank\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input parameters.\n",
    "One of the most important parameters is the number of estimators that allows you to trade-off accuracy, modeling time, and model size. The table below should give you an idea of the relationships between the number of estimators and the metrics.\n",
    "\n",
    "| Estimators | Run time (s) | Size (MB) | Accuracy@1 | Accuracy@2 | Accuracy@3 |\n",
    "|------------|--------------|-----------|------------|------------|------------|\n",
    "|        100 |           40 |  2 | 25.02% | 38.72% | 47.83% |\n",
    "|       1000 |          177 |  4 | 46.79% | 60.80% | 69.11% |\n",
    "|       2000 |          359 |  7 | 51.38% | 65.93% | 73.09% |\n",
    "|       4000 |          628 | 12 | 53.39% | 67.40% | 74.74% |\n",
    "|       8000 |          904 | 22 | 54,62% | 67.77% | 75.35% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train_data = 'balanced_pairs_train.tsv' # The file of training data.\n",
    "args_test_data = 'balanced_pairs_test.tsv'   # The file of testing data.\n",
    "args_estimators = 8000                       # The number of estimators fit by LightGBM.\n",
    "args_min_child_samples = 20                  # The minimum number of samples in a leaf created bty LightGBM.\n",
    "args_verbose = -1                            # The progress report messages from LightGBM; \"-1\" means none.\n",
    "args_ngrams = 1                              # The maximum size of ngrams created by TfidfVectorizer.\n",
    "args_unweighted = False                      # Whether to ignore instance weights used to correct imbalance in training.\n",
    "args_match = 20                              # The maximum number of original questions per duplicate to use in the data. \n",
    "args_outputs = '.'                           # The folder where this notebook deposits its outputs.\n",
    "args_inputs = '.'                            # The folder where this notebook picks up its inputs.\n",
    "args_save = True                             # Whether to save the model created by the notebook.\n",
    "args_model = 'model.pkl'                     # The file containing the saved model.\n",
    "args_instances = 'inst.txt'                  # The file containing the scored test data.\n",
    "args_labels = 'labels.txt'                   # The file containing the ordered unique ids of the original questions's answer ids. \n",
    "args_rank = 3                                # The maximum position at which to report test set accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths to the notebook's input and output files\n",
    "\n",
    "The training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_path = args_inputs\n",
    "data_path = os.path.join(inputs_path, args_train_data)\n",
    "test_path = os.path.join(inputs_path, args_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model file and the scored test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_path = args_outputs\n",
    "os.makedirs(outputs_path, exist_ok=True)                     # Create the outputs folder.\n",
    "model_path = os.path.join(outputs_path, args_model)\n",
    "instances_path = os.path.join(outputs_path, args_instances)\n",
    "labels_path = os.path.join(outputs_path, args_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and set up the training data\n",
    "\n",
    "Load the training data, and display a sample of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading {}'.format(data_path))\n",
    "train = pd.read_csv(data_path, sep='\\t', encoding='latin1')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit the number of duplicate-original question matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.n < args_match]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the roles of the columns in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['Text_x', 'Text_y']\n",
    "label_column = 'Label'\n",
    "duplicates_id_column = 'Id_x'\n",
    "answer_id_column = 'AnswerId_y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report on the training dataset: the number of rows and the proportion of true matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train: {:,} rows with {:.2%} matches'.format(\n",
    "      train.shape[0], train[label_column].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the instance weights used to correct for class imbalance in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_column = 'Weight'\n",
    "if args_unweighted:\n",
    "    weight = pd.Series([1.0], train[label_column].unique())\n",
    "else:\n",
    "    label_counts = train[label_column].value_counts()\n",
    "    weight = train.shape[0]/(label_counts.shape[0]*label_counts)\n",
    "train[weight_column] = train[label_column].apply(lambda x: weight[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the unique ids that identify each original question's answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(train[answer_id_column].unique())\n",
    "label_order = pd.DataFrame({'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model.\n",
    "\n",
    "Collect the parts of the training data by role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[feature_columns]\n",
    "train_y = train[label_column]\n",
    "sample_weight = train[weight_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the inputs to define the hyperparameters used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = args_estimators\n",
    "min_child_samples = args_min_child_samples\n",
    "if args_ngrams > 0:\n",
    "    ngram_range = (1, args_ngrams)\n",
    "else:\n",
    "    ngram_range = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the hyperparameter values are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_estimators > 0\n",
    "assert min_child_samples > 1\n",
    "assert type(ngram_range) is tuple and len(ngram_range) == 2\n",
    "assert ngram_range[0] > 0 and ngram_range[0] <= ngram_range[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pipeline that featurizes the text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurization = [\n",
    "    (column,\n",
    "     make_pipeline(ItemSelector(column),\n",
    "                   text.TfidfVectorizer(ngram_range=ngram_range)))\n",
    "    for column in feature_columns]\n",
    "features = FeatureUnion(featurization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the estimator that learns how to classify duplicate-original question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = lgb.LGBMClassifier(n_estimators=n_estimators,\n",
    "                               min_child_samples=min_child_samples,\n",
    "                               verbose=args_verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model pipeline as feeding the features into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('features', features),\n",
    "    ('model', estimator)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model.\n",
    "This step should take about seven and a half minutes on a Standard NC6 DLVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(train_X, train_y, model__sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file, and report on its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_save:\n",
    "    joblib.dump(model, model_path)\n",
    "    print('{} size: {:.2f} MB'.format(model_path, os.path.getsize(model_path)/(2**20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Read in the test data set, and report of the number of its rows and proportion of true matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading {}'.format(test_path))\n",
    "test = pd.read_csv(test_path, sep='\\t', encoding='latin1')\n",
    "print('test {:,} rows with {:.2%} matches'.format(\n",
    "    test.shape[0], test[label_column].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a sample of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the model predictions. This step should take about 1 minute on a Standard NC6 DLVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_X = test[feature_columns]\n",
    "test['probabilities'] = model.predict_proba(test_X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the sample with the added probabilities column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the probabilities for each duplicate question, ordered by the original question ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the testing data by duplicate question id and original question id.\n",
    "test.sort_values([duplicates_id_column, answer_id_column], inplace=True)\n",
    "\n",
    "# Extract the ordered probabilities.\n",
    "probabilities = (\n",
    "    test.probabilities\n",
    "    .groupby(test[duplicates_id_column], sort=False)\n",
    "    .apply(lambda x: tuple(x.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame with one row per duplicate question, and make it contain the model's predictions for each duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = (test[['Id_x', 'AnswerId_x', 'Text_x']]\n",
    "              .drop_duplicates()\n",
    "              .set_index(duplicates_id_column))\n",
    "test_score['probabilities'] = probabilities\n",
    "test_score.reset_index(inplace=True)\n",
    "test_score.columns = ['Id', 'AnswerId', 'Text', 'probabilities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a sample of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the predictions\n",
    "\n",
    "For each duplicate question, find the rank of its correct original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score['Ranks'] = test_score.apply(lambda x:\n",
    "                                       label_rank(x.AnswerId,\n",
    "                                                  x.probabilities,\n",
    "                                                  label_order.label),\n",
    "                                       axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the fraction of correct original questions by minimum rank. Also print the average rank of the correct original questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, args_rank+1):\n",
    "    print('Accuracy @{} = {:.2%}'.format(\n",
    "        i, (test_score['Ranks'] <= i).mean()))\n",
    "mean_rank = test_score['Ranks'].mean()\n",
    "print('Mean Rank {:.4f}'.format(mean_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the scored instances to a file, along with the ordered original questions's answer ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score.to_csv(instances_path, sep='\\t', index=False,\n",
    "                  encoding='latin1')\n",
    "label_order.to_csv(labels_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will [develop the model API](02_Develop_Model_Driver.ipynb) to call our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployment]",
   "language": "python",
   "name": "conda-env-MLAKSDeployment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
