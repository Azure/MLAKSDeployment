{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Web App on Azure Container Services (AKS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will set up an Azure Container Service which will be managed by Kubernetes. We will then take the Docker image we created earlier that contains our app and deploy it to the AKS cluster. Then, we will check everything is working by sending a question to it and getting it scored for matches in the original questions.\n",
    "\n",
    "The process is split into the following steps:\n",
    "- Define our resource names\n",
    "- Login to Azure\n",
    "- Create resource group and create AKS\n",
    "- Connect to AKS\n",
    "- Deploy our app\n",
    "\n",
    "We assume that this notebook is running on Linux and Azure CLI is installed before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from utilities import write_json_to_file\n",
    "%load_ext dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the various name definitions for the resources needed to setup AKS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append .env\n",
    "# This cell is tagged `parameters`\n",
    "# Please modify the values below as you see fit\n",
    "\n",
    "# If you have multiple subscriptions select the subscription you want to use \n",
    "selected_subscription = \"YOUR_SUBSCRIPTION\"\n",
    "\n",
    "# Resource group, name and location for AKS cluster.\n",
    "resource_group = \"RESOURCE_GROUP\" \n",
    "aks_name = \"AKS_CLUSTER_NAME\"\n",
    "location = \"eastus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile --append .env\n",
    "\n",
    "# This cell is tagged `parameters`\n",
    "# Please modify the values below as you see fit\n",
    "\n",
    "# If you have multiple subscriptions select the subscription you want to use \n",
    "selected_subscription = \"Team Danielle Internal\"\n",
    "\n",
    "# Resource group, name and location for AKS cluster.\n",
    "resource_group = \"fbmlakstestrg\" \n",
    "aks_name = \"fbmlaksclus\"\n",
    "location = \"eastus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv\n",
    "image_name = os.getenv('docker_login') + os.getenv('image_repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caia/mlaksdep'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure account login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will initiate a login to your Azure account. It will pop up with an url to go to where you will enter a one off code and log into your Azure account using your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code FPHSBG9T2 to authenticate.\u001b[0m\n",
      "CloudName    IsDefault    Name                                            State    TenantId\n",
      "-----------  -----------  ----------------------------------------------  -------  ------------------------------------\n",
      "AzureCloud   False        Boston DS Dev                                   Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   False        Internal Consumption                            Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   True         Team Danielle Internal                          Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   False        Azure Stack Diagnostics CI and Production VaaS  Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   False        Core-ES-BLD                                     Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   False        Cosmos_WDG_Core_BnB_100348                      Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   False        Data Wrangling Preview                          Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   False        AzureCAT WWAHAIHoL                              Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   False        Solution Template Testing                       Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "AzureCloud   False        Team Ilan                                       Enabled  72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "!az login -o table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account set --subscription \"$selected_subscription\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"environmentName\": \"AzureCloud\",\r\n",
      "  \"id\": \"edf507a2-6235-46c5-b560-fd463ba2e771\",\r\n",
      "  \"isDefault\": true,\r\n",
      "  \"name\": \"Team Danielle Internal\",\r\n",
      "  \"state\": \"Enabled\",\r\n",
      "  \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\r\n",
      "  \"user\": {\r\n",
      "    \"name\": \"fboylu@microsoft.com\",\r\n",
      "    \"type\": \"user\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!az account show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to register the container service resources on your subscription if you haven't already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az provider register -n Microsoft.ContainerService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az provider show -n Microsoft.ContainerService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create resources and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create resource group and AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure encourages the use of groups to organize all the Azure components you deploy. That way it is easier to find them but also we can delete a number of resources simply by deleting the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/fbmlakstestrg\",\r\n",
      "  \"location\": \"eastus\",\r\n",
      "  \"managedBy\": null,\r\n",
      "  \"name\": \"fbmlakstestrg\",\r\n",
      "  \"properties\": {\r\n",
      "    \"provisioningState\": \"Succeeded\"\r\n",
      "  },\r\n",
      "  \"tags\": null\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!az group create --name $resource_group --location $location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create the AKS cluster  with 5 nodes in the resource group we created earlier. This step can take ten or more minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K{- Finished ..\n",
      "  \"agentPoolProfiles\": [\n",
      "    {\n",
      "      \"count\": 5,\n",
      "      \"dnsPrefix\": null,\n",
      "      \"fqdn\": null,\n",
      "      \"name\": \"nodepool1\",\n",
      "      \"osDiskSizeGb\": null,\n",
      "      \"osType\": \"Linux\",\n",
      "      \"ports\": null,\n",
      "      \"storageProfile\": \"ManagedDisks\",\n",
      "      \"vmSize\": \"Standard_D4_v2\",\n",
      "      \"vnetSubnetId\": null\n",
      "    }\n",
      "  ],\n",
      "  \"dnsPrefix\": \"fbmlaksclu-fbmlakstestrg-edf507\",\n",
      "  \"fqdn\": \"fbmlaksclu-fbmlakstestrg-edf507-8c3abe83.hcp.eastus.azmk8s.io\",\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourcegroups/fbmlakstestrg/providers/Microsoft.ContainerService/managedClusters/fbmlaksclus\",\n",
      "  \"kubernetesVersion\": \"1.9.9\",\n",
      "  \"linuxProfile\": {\n",
      "    \"adminUsername\": \"azureuser\",\n",
      "    \"ssh\": {\n",
      "      \"publicKeys\": [\n",
      "        {\n",
      "          \"keyData\": \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTZYQFHNstYCR25qtvMrC6baTMS6TobaIRbgd0xOoafDy+2uBk0DMJuhGWoOcrsCnvadp5k/0K8qBRysyhlQGWb6+r8fBunThy+zpTKqdh3W8Q1y5UtKnGwwU1cqGXDOPUIXJYNPJqUKV829+MOrZjUynhHgSzDbY2ncGyoT+Farsvm01aGEdDapa+XRl4JAwtN1bb9q+Ii5y+MkpIOhLRMwATl05eNfAHmYQWtaIJZZJOHMNPswlBmLs293Wsj11vYh6/yo9S4ToEsc9Pbl5Zn6OFIu7jfzN2bM8cA3+8pru9WSthrxjJvPn8i4uTYozOdNIi09ArQ4lRT9t6rsMz\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"location\": \"eastus\",\n",
      "  \"name\": \"fbmlaksclus\",\n",
      "  \"provisioningState\": \"Succeeded\",\n",
      "  \"resourceGroup\": \"fbmlakstestrg\",\n",
      "  \"servicePrincipalProfile\": {\n",
      "    \"clientId\": \"eeba3bfe-45f9-42de-9bdb-54416b67382d\",\n",
      "    \"keyVaultSecretRef\": null,\n",
      "    \"secret\": null\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": \"Microsoft.ContainerService/ManagedClusters\"\n",
      "}\n",
      "\u001b[0mCPU times: user 9.39 s, sys: 1.42 s, total: 10.8 s\n",
      "Wall time: 13min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!az aks create --resource-group $resource_group --name $aks_name --node-count 5 --generate-ssh-keys -s Standard_D4_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install kubectl CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the Kubernetes cluster, we will use kubectl, the Kubernetes command-line client. To install, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading client to /usr/local/bin/kubectl from https://storage.googleapis.com/kubernetes-release/release/v1.11.2/bin/linux/amd64/kubectl\u001b[0m\n",
      "\u001b[33mPlease ensure that /usr/local/bin is in your search PATH, so the `kubectl` command can be found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo env \"PATH=$PATH\" az aks install-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure kubectl to connect to the Kubernetes cluster, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged \"fbmlaksclus\" as current context in /home/fboylu/.kube/config\r\n"
     ]
    }
   ],
   "source": [
    "!az aks get-credentials --resource-group $resource_group --name $aks_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify connection by listing the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS    ROLES     AGE       VERSION\r\n",
      "aks-nodepool1-34598344-0   Ready     agent     8m        v1.9.9\r\n",
      "aks-nodepool1-34598344-1   Ready     agent     9m        v1.9.9\r\n",
      "aks-nodepool1-34598344-2   Ready     agent     9m        v1.9.9\r\n",
      "aks-nodepool1-34598344-3   Ready     agent     9m        v1.9.9\r\n",
      "aks-nodepool1-34598344-4   Ready     agent     9m        v1.9.9\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the pods on our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\r\n",
      "kube-system   azureproxy-7c677567f6-tjb84             1/1       Running   3          8m\r\n",
      "kube-system   heapster-6fbcd8c68f-sbghs               2/2       Running   0          6m\r\n",
      "kube-system   kube-dns-v20-784dd5dbd5-jvzh6           3/3       Running   0          8m\r\n",
      "kube-system   kube-dns-v20-784dd5dbd5-php9q           3/3       Running   0          8m\r\n",
      "kube-system   kube-proxy-4pqrn                        1/1       Running   0          8m\r\n",
      "kube-system   kube-proxy-hj5c5                        1/1       Running   0          8m\r\n",
      "kube-system   kube-proxy-msrqn                        1/1       Running   0          8m\r\n",
      "kube-system   kube-proxy-rrktt                        1/1       Running   0          8m\r\n",
      "kube-system   kube-proxy-w8bp6                        1/1       Running   0          8m\r\n",
      "kube-system   kube-svc-redirect-46whz                 1/1       Running   0          8m\r\n",
      "kube-system   kube-svc-redirect-82hzk                 1/1       Running   0          8m\r\n",
      "kube-system   kube-svc-redirect-l42q7                 1/1       Running   0          8m\r\n",
      "kube-system   kube-svc-redirect-vnmrj                 1/1       Running   0          8m\r\n",
      "kube-system   kube-svc-redirect-w7n69                 1/1       Running   0          8m\r\n",
      "kube-system   kubernetes-dashboard-7bb7584f55-l4m88   1/1       Running   1          8m\r\n",
      "kube-system   tunnelfront-5d776cdcfd-2gzvz            1/1       Running   0          8m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define our Kubernetes manifest file for our service and load balancer. Note that we have to specify the image name and cpu requests and limits for pods. We first start with  deploying 2 pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_template = {\n",
    "  \"apiVersion\": \"apps/v1beta1\",\n",
    "  \"kind\": \"Deployment\",\n",
    "  \"metadata\": {\n",
    "      \"name\": \"azure-ml\"\n",
    "  },\n",
    "  \"spec\":{\n",
    "      \"replicas\":2,\n",
    "      \"template\":{\n",
    "          \"metadata\":{\n",
    "              \"labels\":{\n",
    "                  \"app\":\"azure-ml\"\n",
    "              }\n",
    "          },\n",
    "          \"spec\":{\n",
    "              \"containers\":[\n",
    "                  {\n",
    "                      \"name\": \"azure-ml\",\n",
    "                      \"image\": image_name,\n",
    "\n",
    "                      \"ports\":[\n",
    "                          {\n",
    "                              \"containerPort\":80,\n",
    "                              \"name\":\"model\"\n",
    "                          }\n",
    "                      ],\n",
    "                      \"resources\":{\n",
    "                           \"requests\":{\n",
    "                               \"cpu\": 1\n",
    "                           },\n",
    "                           \"limits\":{\n",
    "                               \"cpu\": 1.25\n",
    "                           }\n",
    "                       }  \n",
    "                  }\n",
    "              ]\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "service_temp = {\n",
    "  \"apiVersion\": \"v1\",\n",
    "  \"kind\": \"Service\",\n",
    "  \"metadata\": {\n",
    "      \"name\": \"azure-ml\"\n",
    "  },\n",
    "  \"spec\":{\n",
    "      \"type\": \"LoadBalancer\",\n",
    "      \"ports\":[\n",
    "          {\n",
    "              \"port\":80\n",
    "          }\n",
    "      ],\n",
    "      \"selector\":{\n",
    "            \"app\":\"azure-ml\"\n",
    "      }\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(app_template, 'az-ml.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(service_temp, 'az-ml.json', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the manifest created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"apiVersion\": \"apps/v1beta1\",\r\n",
      "    \"kind\": \"Deployment\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"name\": \"azure-ml\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"replicas\": 2,\r\n",
      "        \"template\": {\r\n",
      "            \"metadata\": {\r\n",
      "                \"labels\": {\r\n",
      "                    \"app\": \"azure-ml\"\r\n",
      "                }\r\n",
      "            },\r\n",
      "            \"spec\": {\r\n",
      "                \"containers\": [\r\n",
      "                    {\r\n",
      "                        \"image\": \"caia/mlaksdep\",\r\n",
      "                        \"name\": \"azure-ml\",\r\n",
      "                        \"ports\": [\r\n",
      "                            {\r\n",
      "                                \"containerPort\": 80,\r\n",
      "                                \"name\": \"model\"\r\n",
      "                            }\r\n",
      "                        ],\r\n",
      "                        \"resources\": {\r\n",
      "                            \"limits\": {\r\n",
      "                                \"cpu\": 1.25\r\n",
      "                            },\r\n",
      "                            \"requests\": {\r\n",
      "                                \"cpu\": 1\r\n",
      "                            }\r\n",
      "                        }\r\n",
      "                    }\r\n",
      "                ]\r\n",
      "            }\r\n",
      "        }\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n",
      "{\r\n",
      "    \"apiVersion\": \"v1\",\r\n",
      "    \"kind\": \"Service\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"name\": \"azure-ml\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"ports\": [\r\n",
      "            {\r\n",
      "                \"port\": 80\r\n",
      "            }\r\n",
      "        ],\r\n",
      "        \"selector\": {\r\n",
      "            \"app\": \"azure-ml\"\r\n",
      "        },\r\n",
      "        \"type\": \"LoadBalancer\"\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat az-ml.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use kubectl create command to deploy our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/azure-ml created\n",
      "service/azure-ml created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f az-ml.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the pod is deployed. It can take several minutes for the deployment to be ready and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\r\n",
      "default       azure-ml-7d4d8c754b-dvktm               1/1       Running   0          18m\r\n",
      "default       azure-ml-7d4d8c754b-hf97s               1/1       Running   0          18m\r\n",
      "kube-system   azureproxy-7c677567f6-tjb84             1/1       Running   3          27m\r\n",
      "kube-system   heapster-6fbcd8c68f-sbghs               2/2       Running   0          25m\r\n",
      "kube-system   kube-dns-v20-784dd5dbd5-jvzh6           3/3       Running   0          27m\r\n",
      "kube-system   kube-dns-v20-784dd5dbd5-php9q           3/3       Running   0          27m\r\n",
      "kube-system   kube-proxy-4pqrn                        1/1       Running   0          27m\r\n",
      "kube-system   kube-proxy-hj5c5                        1/1       Running   0          27m\r\n",
      "kube-system   kube-proxy-msrqn                        1/1       Running   0          27m\r\n",
      "kube-system   kube-proxy-rrktt                        1/1       Running   0          27m\r\n",
      "kube-system   kube-proxy-w8bp6                        1/1       Running   0          27m\r\n",
      "kube-system   kube-svc-redirect-46whz                 1/1       Running   0          27m\r\n",
      "kube-system   kube-svc-redirect-82hzk                 1/1       Running   0          27m\r\n",
      "kube-system   kube-svc-redirect-l42q7                 1/1       Running   0          27m\r\n",
      "kube-system   kube-svc-redirect-vnmrj                 1/1       Running   0          27m\r\n",
      "kube-system   kube-svc-redirect-w7n69                 1/1       Running   0          27m\r\n",
      "kube-system   kubernetes-dashboard-7bb7584f55-l4m88   1/1       Running   1          27m\r\n",
      "kube-system   tunnelfront-5d776cdcfd-2gzvz            1/1       Running   0          27m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything goes wrong you can use the commands below to observe the events on the node as well as review the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   FIRST SEEN   COUNT     NAME                                         KIND         SUBOBJECT                   TYPE      REASON                    SOURCE                                 MESSAGE\r\n",
      "29m         29m          1         aks-nodepool1-34598344-0.154901c0026d3a54    Node                                     Normal    Starting                  kubelet, aks-nodepool1-34598344-0      Starting kubelet.\r\n",
      "28m         29m          8         aks-nodepool1-34598344-0.154901c0044b32a1    Node                                     Normal    NodeHasSufficientDisk     kubelet, aks-nodepool1-34598344-0      Node aks-nodepool1-34598344-0 status is now: NodeHasSufficientDisk\r\n",
      "28m         29m          8         aks-nodepool1-34598344-0.154901c0044b5e61    Node                                     Normal    NodeHasSufficientMemory   kubelet, aks-nodepool1-34598344-0      Node aks-nodepool1-34598344-0 status is now: NodeHasSufficientMemory\r\n",
      "28m         29m          7         aks-nodepool1-34598344-0.154901c0044b7c11    Node                                     Normal    NodeHasNoDiskPressure     kubelet, aks-nodepool1-34598344-0      Node aks-nodepool1-34598344-0 status is now: NodeHasNoDiskPressure\r\n",
      "29m         29m          1         aks-nodepool1-34598344-0.154901c0052197d3    Node                                     Normal    NodeAllocatableEnforced   kubelet, aks-nodepool1-34598344-0      Updated Node Allocatable limit across pods\r\n",
      "27m         27m          1         aks-nodepool1-34598344-0.154901d7ce16c557    Node                                     Normal    RegisteredNode            node-controller                        Node aks-nodepool1-34598344-0 event: Registered Node aks-nodepool1-34598344-0 in Controller\r\n",
      "26m         26m          1         aks-nodepool1-34598344-0.154901e5b141011a    Node                                     Normal    Starting                  kube-proxy, aks-nodepool1-34598344-0   Starting kube-proxy.\r\n",
      "29m         29m          1         aks-nodepool1-34598344-1.154901bce3031001    Node                                     Normal    Starting                  kubelet, aks-nodepool1-34598344-1      Starting kubelet.\r\n",
      "28m         29m          8         aks-nodepool1-34598344-1.154901bce4aef509    Node                                     Normal    NodeHasSufficientDisk     kubelet, aks-nodepool1-34598344-1      Node aks-nodepool1-34598344-1 status is now: NodeHasSufficientDisk\r\n",
      "28m         29m          8         aks-nodepool1-34598344-1.154901bce4af5073    Node                                     Normal    NodeHasSufficientMemory   kubelet, aks-nodepool1-34598344-1      Node aks-nodepool1-34598344-1 status is now: NodeHasSufficientMemory\r\n",
      "28m         29m          7         aks-nodepool1-34598344-1.154901bce4af7463    Node                                     Normal    NodeHasNoDiskPressure     kubelet, aks-nodepool1-34598344-1      Node aks-nodepool1-34598344-1 status is now: NodeHasNoDiskPressure\r\n",
      "29m         29m          1         aks-nodepool1-34598344-1.154901bce589f9dc    Node                                     Normal    NodeAllocatableEnforced   kubelet, aks-nodepool1-34598344-1      Updated Node Allocatable limit across pods\r\n",
      "27m         27m          1         aks-nodepool1-34598344-1.154901d7ce149aa8    Node                                     Normal    RegisteredNode            node-controller                        Node aks-nodepool1-34598344-1 event: Registered Node aks-nodepool1-34598344-1 in Controller\r\n",
      "27m         27m          1         aks-nodepool1-34598344-1.154901e2ff1a0a59    Node                                     Normal    Starting                  kube-proxy, aks-nodepool1-34598344-1   Starting kube-proxy.\r\n",
      "29m         29m          1         aks-nodepool1-34598344-2.154901bab93a487b    Node                                     Normal    Starting                  kubelet, aks-nodepool1-34598344-2      Starting kubelet.\r\n",
      "28m         29m          8         aks-nodepool1-34598344-2.154901babaff8d86    Node                                     Normal    NodeHasSufficientDisk     kubelet, aks-nodepool1-34598344-2      Node aks-nodepool1-34598344-2 status is now: NodeHasSufficientDisk\r\n",
      "28m         29m          8         aks-nodepool1-34598344-2.154901babaffbebe    Node                                     Normal    NodeHasSufficientMemory   kubelet, aks-nodepool1-34598344-2      Node aks-nodepool1-34598344-2 status is now: NodeHasSufficientMemory\r\n",
      "28m         29m          7         aks-nodepool1-34598344-2.154901babaffe183    Node                                     Normal    NodeHasNoDiskPressure     kubelet, aks-nodepool1-34598344-2      Node aks-nodepool1-34598344-2 status is now: NodeHasNoDiskPressure\r\n",
      "29m         29m          1         aks-nodepool1-34598344-2.154901babbc534db    Node                                     Normal    NodeAllocatableEnforced   kubelet, aks-nodepool1-34598344-2      Updated Node Allocatable limit across pods\r\n",
      "27m         27m          1         aks-nodepool1-34598344-2.154901d7ce1695af    Node                                     Normal    RegisteredNode            node-controller                        Node aks-nodepool1-34598344-2 event: Registered Node aks-nodepool1-34598344-2 in Controller\r\n",
      "27m         27m          1         aks-nodepool1-34598344-2.154901e27122c930    Node                                     Normal    Starting                  kube-proxy, aks-nodepool1-34598344-2   Starting kube-proxy.\r\n",
      "29m         29m          1         aks-nodepool1-34598344-3.154901c27237de2f    Node                                     Normal    Starting                  kubelet, aks-nodepool1-34598344-3      Starting kubelet.\r\n",
      "28m         29m          8         aks-nodepool1-34598344-3.154901c273bb3392    Node                                     Normal    NodeHasSufficientDisk     kubelet, aks-nodepool1-34598344-3      Node aks-nodepool1-34598344-3 status is now: NodeHasSufficientDisk\r\n",
      "28m         29m          8         aks-nodepool1-34598344-3.154901c273bb64ca    Node                                     Normal    NodeHasSufficientMemory   kubelet, aks-nodepool1-34598344-3      Node aks-nodepool1-34598344-3 status is now: NodeHasSufficientMemory\r\n",
      "28m         29m          7         aks-nodepool1-34598344-3.154901c273bb89e6    Node                                     Normal    NodeHasNoDiskPressure     kubelet, aks-nodepool1-34598344-3      Node aks-nodepool1-34598344-3 status is now: NodeHasNoDiskPressure\r\n",
      "29m         29m          1         aks-nodepool1-34598344-3.154901c27490faa0    Node                                     Normal    NodeAllocatableEnforced   kubelet, aks-nodepool1-34598344-3      Updated Node Allocatable limit across pods\r\n",
      "27m         27m          1         aks-nodepool1-34598344-3.154901d7ce159ed0    Node                                     Normal    RegisteredNode            node-controller                        Node aks-nodepool1-34598344-3 event: Registered Node aks-nodepool1-34598344-3 in Controller\r\n",
      "27m         27m          1         aks-nodepool1-34598344-3.154901d8347fe8fd    Node                                     Warning   FailedToCreateRoute       route_controller                       Could not create route 63f5b357-9b47-11e8-ab24-46cd4c7ad070 10.244.1.0/24 for node aks-nodepool1-34598344-3 after 1.270801031s: network.RoutesClient#CreateOrUpdate: Failure responding to request: StatusCode=429 -- Original Error: autorest/azure: Service returned an error. Status=429 Code=\"RetryableError\" Message=\"A retryable error occurred.\" Details=[{\"code\":\"RetryableErrorDueToAnotherOperation\",\"message\":\"Operation PutRouteOperation (3941b65b-b06c-4c54-b1df-04dc17c66b0a) is updating resource /subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/MC_fbmlakstestrg_fbmlaksclus_eastus/providers/Microsoft.Network/routeTables/aks-agentpool-34598344-routetable. The call can be retried in 11 seconds.\"}]\r\n",
      "26m         26m          1         aks-nodepool1-34598344-3.154901e4c162034a    Node                                     Normal    Starting                  kube-proxy, aks-nodepool1-34598344-3   Starting kube-proxy.\r\n",
      "29m         29m          1         aks-nodepool1-34598344-4.154901c386a22b61    Node                                     Normal    Starting                  kubelet, aks-nodepool1-34598344-4      Starting kubelet.\r\n",
      "28m         29m          8         aks-nodepool1-34598344-4.154901c388b5741a    Node                                     Normal    NodeHasSufficientDisk     kubelet, aks-nodepool1-34598344-4      Node aks-nodepool1-34598344-4 status is now: NodeHasSufficientDisk\r\n",
      "28m         29m          8         aks-nodepool1-34598344-4.154901c388b5df23    Node                                     Normal    NodeHasSufficientMemory   kubelet, aks-nodepool1-34598344-4      Node aks-nodepool1-34598344-4 status is now: NodeHasSufficientMemory\r\n",
      "28m         29m          7         aks-nodepool1-34598344-4.154901c388b60ecb    Node                                     Normal    NodeHasNoDiskPressure     kubelet, aks-nodepool1-34598344-4      Node aks-nodepool1-34598344-4 status is now: NodeHasNoDiskPressure\r\n",
      "29m         29m          1         aks-nodepool1-34598344-4.154901c389ac8761    Node                                     Normal    NodeAllocatableEnforced   kubelet, aks-nodepool1-34598344-4      Updated Node Allocatable limit across pods\r\n",
      "27m         27m          1         aks-nodepool1-34598344-4.154901d7ce16ad1f    Node                                     Normal    RegisteredNode            node-controller                        Node aks-nodepool1-34598344-4 event: Registered Node aks-nodepool1-34598344-4 in Controller\r\n",
      "26m         26m          1         aks-nodepool1-34598344-4.154901ea8bdfbd51    Node                                     Normal    Starting                  kube-proxy, aks-nodepool1-34598344-4   Starting kube-proxy.\r\n",
      "18m         18m          1         azure-ml-7d4d8c754b-dvktm.15490257926c852f   Pod                                      Normal    Scheduled                 default-scheduler                      Successfully assigned azure-ml-7d4d8c754b-dvktm to aks-nodepool1-34598344-2\r\n",
      "18m         18m          1         azure-ml-7d4d8c754b-dvktm.15490257a12f1337   Pod                                      Normal    SuccessfulMountVolume     kubelet, aks-nodepool1-34598344-2      MountVolume.SetUp succeeded for volume \"default-token-pbdph\" \r\n",
      "18m         18m          1         azure-ml-7d4d8c754b-dvktm.15490257e49b13e3   Pod          spec.containers{azure-ml}   Normal    Pulling                   kubelet, aks-nodepool1-34598344-2      pulling image \"caia/mlaksdep\"\r\n",
      "11m         11m          1         azure-ml-7d4d8c754b-dvktm.154902bbc73afb09   Pod          spec.containers{azure-ml}   Normal    Pulled                    kubelet, aks-nodepool1-34598344-2      Successfully pulled image \"caia/mlaksdep\"\r\n",
      "11m         11m          1         azure-ml-7d4d8c754b-dvktm.154902bd0d3a1a7d   Pod          spec.containers{azure-ml}   Normal    Created                   kubelet, aks-nodepool1-34598344-2      Created container\r\n",
      "11m         11m          1         azure-ml-7d4d8c754b-dvktm.154902bd1b1f4e71   Pod          spec.containers{azure-ml}   Normal    Started                   kubelet, aks-nodepool1-34598344-2      Started container\r\n",
      "18m         18m          1         azure-ml-7d4d8c754b-hf97s.1549025794935004   Pod                                      Normal    Scheduled                 default-scheduler                      Successfully assigned azure-ml-7d4d8c754b-hf97s to aks-nodepool1-34598344-0\r\n",
      "18m         18m          1         azure-ml-7d4d8c754b-hf97s.15490257a472c69d   Pod                                      Normal    SuccessfulMountVolume     kubelet, aks-nodepool1-34598344-0      MountVolume.SetUp succeeded for volume \"default-token-pbdph\" \r\n",
      "18m         18m          1         azure-ml-7d4d8c754b-hf97s.15490257f8c33b76   Pod          spec.containers{azure-ml}   Normal    Pulling                   kubelet, aks-nodepool1-34598344-0      pulling image \"caia/mlaksdep\"\r\n",
      "11m         11m          1         azure-ml-7d4d8c754b-hf97s.154902be69e75f73   Pod          spec.containers{azure-ml}   Normal    Pulled                    kubelet, aks-nodepool1-34598344-0      Successfully pulled image \"caia/mlaksdep\"\r\n",
      "11m         11m          1         azure-ml-7d4d8c754b-hf97s.154902bfa2ca9fef   Pod          spec.containers{azure-ml}   Normal    Created                   kubelet, aks-nodepool1-34598344-0      Created container\r\n",
      "11m         11m          1         azure-ml-7d4d8c754b-hf97s.154902bfaddc46f3   Pod          spec.containers{azure-ml}   Normal    Started                   kubelet, aks-nodepool1-34598344-0      Started container\r\n",
      "18m         18m          1         azure-ml-7d4d8c754b.154902578dfbece3         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-ml-7d4d8c754b-dvktm\r\n",
      "18m         18m          1         azure-ml-7d4d8c754b.15490257928c9f80         ReplicaSet                               Normal    SuccessfulCreate          replicaset-controller                  Created pod: azure-ml-7d4d8c754b-hf97s\r\n",
      "18m         18m          1         azure-ml.154902578c42f20f                    Deployment                               Normal    ScalingReplicaSet         deployment-controller                  Scaled up replica set azure-ml-7d4d8c754b to 2\r\n",
      "18m         18m          1         azure-ml.1549025794973f77                    Service                                  Normal    EnsuringLoadBalancer      service-controller                     Ensuring load balancer\r\n",
      "15m         15m          1         azure-ml.15490285b7619293                    Service                                  Normal    EnsuredLoadBalancer       service-controller                     Ensured load balancer\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the logs for the first application pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_json = !kubectl get pods -o json\n",
    "pod_dict = json.loads(''.join(pod_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-08 20:29:19,353 CRIT Supervisor running as root (no user in config file)\r\n",
      "2018-08-08 20:29:19,355 INFO supervisord started with pid 1\r\n",
      "2018-08-08 20:29:20,357 INFO spawned: 'program_exit' with pid 9\r\n",
      "2018-08-08 20:29:20,358 INFO spawned: 'nginx' with pid 10\r\n",
      "2018-08-08 20:29:20,360 INFO spawned: 'gunicorn' with pid 11\r\n",
      "2018-08-08 20:29:21,392 INFO success: program_exit entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\r\n",
      "{\"stack_info\": null, \"tags\": [], \"level\": \"INFO\", \"logger\": \"model_driver\", \"message\": \"Model object loading time: 808.79 ms\", \"timestamp\": \"2018-08-08T20:29:21.978340Z\", \"host\": \"azure-ml-7d4d8c754b-dvktm\", \"path\": \"/code/driver.py\"}\r\n",
      "Initialising\r\n",
      "{\"msg\": \" * Running on %s://%s:%d/ %s\", \"stack_info\": null, \"tags\": [], \"level\": \"INFO\", \"logger\": \"werkzeug\", \"message\": \" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\", \"timestamp\": \"2018-08-08T20:29:21.984673Z\", \"host\": \"azure-ml-7d4d8c754b-dvktm\", \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/werkzeug/_internal.py\"}\r\n",
      "2018-08-08 20:29:26,076 INFO success: nginx entered RUNNING state, process has stayed up for > than 5 seconds (startsecs)\r\n",
      "2018-08-08 20:29:41,095 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 20 seconds (startsecs)\r\n",
      "{\"stack_info\": null, \"tags\": [], \"level\": \"INFO\", \"logger\": \"werkzeug\", \"message\": \"127.0.0.1 - - [08/Aug/2018 20:29:50] \\\"GET / HTTP/1.0\\\" 200 -\", \"timestamp\": \"2018-08-08T20:29:50.420876Z\", \"host\": \"azure-ml-7d4d8c754b-dvktm\", \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/werkzeug/_internal.py\"}\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs {pod_dict['items'][0]['metadata']['name']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "azure-ml   2         2         2            2           19m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take a few minutes for the service to populate the EXTERNAL-IP field below. This will be the IP you use to call the service. You can also specify an IP to use, please see the AKS documentation for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       TYPE           CLUSTER-IP   EXTERNAL-IP      PORT(S)        AGE\r\n",
      "azure-ml   LoadBalancer   10.0.9.28    40.117.112.173   80:32091/TCP   19m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service azure-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we scale the number of pods to make sure we fully utilize the AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.extensions/azure-ml scaled\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl scale --current-replicas=2 --replicas=35 deployment/azure-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take a couple of minutes for all replicas to be running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\r\n",
      "default       azure-ml-7d4d8c754b-28cvf               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-2zpnl               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-464d4               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-4fkrp               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-66trz               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-78jtw               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-7nmvm               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-7t5jj               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-99w27               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-blscw               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-d2smj               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-dph8x               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-dvktm               1/1       Running   0          22m\r\n",
      "default       azure-ml-7d4d8c754b-f6n25               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-fjmvg               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-fjpts               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-hf97s               1/1       Running   0          22m\r\n",
      "default       azure-ml-7d4d8c754b-htlr4               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-hxs2m               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-lq4z2               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-mk6db               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-mp64d               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-prglw               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-q4t88               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-qffpd               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-ttgn9               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-vqjrd               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-vrxlx               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-vwjh2               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-wscjg               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-xcffk               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-xdkrx               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-xgwf4               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-xl4f7               1/1       Running   0          3m\r\n",
      "default       azure-ml-7d4d8c754b-xvcll               1/1       Running   0          3m\r\n",
      "kube-system   azureproxy-7c677567f6-tjb84             1/1       Running   3          31m\r\n",
      "kube-system   heapster-6fbcd8c68f-sbghs               2/2       Running   0          29m\r\n",
      "kube-system   kube-dns-v20-784dd5dbd5-jvzh6           3/3       Running   0          31m\r\n",
      "kube-system   kube-dns-v20-784dd5dbd5-php9q           3/3       Running   0          31m\r\n",
      "kube-system   kube-proxy-4pqrn                        1/1       Running   0          31m\r\n",
      "kube-system   kube-proxy-hj5c5                        1/1       Running   0          31m\r\n",
      "kube-system   kube-proxy-msrqn                        1/1       Running   0          31m\r\n",
      "kube-system   kube-proxy-rrktt                        1/1       Running   0          31m\r\n",
      "kube-system   kube-proxy-w8bp6                        1/1       Running   0          31m\r\n",
      "kube-system   kube-svc-redirect-46whz                 1/1       Running   0          31m\r\n",
      "kube-system   kube-svc-redirect-82hzk                 1/1       Running   0          31m\r\n",
      "kube-system   kube-svc-redirect-l42q7                 1/1       Running   0          31m\r\n",
      "kube-system   kube-svc-redirect-vnmrj                 1/1       Running   0          31m\r\n",
      "kube-system   kube-svc-redirect-w7n69                 1/1       Running   0          31m\r\n",
      "kube-system   kubernetes-dashboard-7bb7584f55-l4m88   1/1       Running   1          31m\r\n",
      "kube-system   tunnelfront-5d776cdcfd-2gzvz            1/1       Running   0          31m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "azure-ml   35        35        35           35          22m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will [test our web application deployed on AKS](06_Test_WebApp.ipynb).\n",
    "\n",
    "Once, you are done with all the notebooks of the tutorial, you can use the instructions in the [last notebook](09_Tear_Down.ipynb) to tear down the cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployment]",
   "language": "python",
   "name": "conda-env-MLAKSDeployment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
