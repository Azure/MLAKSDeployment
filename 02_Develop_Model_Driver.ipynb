{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Model Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will develop the API that will call our model. This module initializes the model, transforms the input so that it is in the appropriate format and defines the scoring method that will produce the predictions. The API will expect the input to be in JSON format. Once a request is received, the API will use the request body to score the question text. There are two main functions in the API. The first function loads the model and returns a scoring function. The second function process the question text and uses the first function to score it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from utilities import text_to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the writefile magic to write the contents of the below cell to driver.py which includes the driver methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile driver.py\n",
    "\n",
    "import lightgbm as lgb\n",
    "import timeit as t\n",
    "import logging\n",
    "from duplicate_model import DuplicateModel\n",
    "\n",
    "model_path = 'model.pkl'\n",
    "questions_path = 'questions.tsv'\n",
    "logger = logging.getLogger(\"model_driver\")\n",
    "\n",
    "def create_scoring_func():\n",
    "    \"\"\" Initialize Model Object \n",
    "    \"\"\"   \n",
    "    start = t.default_timer()\n",
    "    DM = DuplicateModel(model_path, questions_path)\n",
    "    end = t.default_timer()\n",
    "    \n",
    "    loadTimeMsg = \"Model object loading time: {0} ms\".format(round((end-start)*1000, 2))\n",
    "    logger.info(loadTimeMsg)\n",
    "    \n",
    "    def call_model(text):\n",
    "        preds = DM.score(text)  \n",
    "        return preds\n",
    "    \n",
    "    return call_model\n",
    "\n",
    "def get_model_api():\n",
    "    logger = logging.getLogger(\"model_driver\")\n",
    "    scoring_func = create_scoring_func()\n",
    "    \n",
    "    def process_and_score(inputString):\n",
    "        \"\"\" Classify the input using the loaded model\n",
    "        \"\"\"\n",
    "        start = t.default_timer()\n",
    "        responses = scoring_func(inputString)\n",
    "        end = t.default_timer()\n",
    "        \n",
    "        logger.info(\"Predictions: {0}\".format(responses))\n",
    "        logger.info(\"Predictions took {0} ms\".format(round((end-start)*1000, 2)))\n",
    "        return (responses, \"Computed in {0} ms\".format(round((end-start)*1000, 2)))\n",
    "    return process_and_score\n",
    "\n",
    "def version():\n",
    "    return lgb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the driver.py which will bring the imports and functions into the context of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run driver.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use one of the duplicate questions to test our driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes_test_path = 'dupes_test.tsv'\n",
    "dupes_test = pd.read_csv(dupes_test_path, sep='\\t', encoding='latin1')\n",
    "text_to_score = dupes_test.iloc[0,4]\n",
    "text_to_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we convert our text for the format that will be required by the Flask application that will use the functions in the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontext = text_to_json(text_to_score)\n",
    "json_load_text = json.loads(jsontext)\n",
    "body = json_load_text['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_for = get_model_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resp = predict_for(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move on to [building our docker image](03_Build_Image.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployment]",
   "language": "python",
   "name": "conda-env-MLAKSDeployment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
